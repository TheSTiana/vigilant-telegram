{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiac-vietnamese",
   "metadata": {},
   "source": [
    "# Fine-tune transformer with  Hugging Face\n",
    "\n",
    "For this part of the project I have explored the Hugging Face library, and the datasets and models available there. While there are many datasets readily available on the site, I decided to use the same dataset of airline tweets which I used with the Fastai library. As such, I hoped to experience the differences loading and preparing the dataset, but I also wanted to do some comparison of the resulting accuracies. As for previous experience with the library, I had none, neither had I any solid experience with transformers. So I stared by working my way through the [Introduction](https://huggingface.co/course/chapter1/1) course.\n",
    "\n",
    "After having considered several different models I found the [Twitter-roBERTa-base for Sentiment Analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) to be a good possible choice. \n",
    "\n",
    "> This is a roBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark.\n",
    "\n",
    "Just as the airline dataset it had three possible classes negative, neutral and positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-chile",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "\n",
    "The Hugging Face library has it own dataset format which is different than the pandas dataframe, which made for a rather tricky start. \n",
    "\n",
    "Those which made the model had also served an function which created placeholders for both username and link, which seemed sensible to apply to the tweets. However, applying a function to a feature in this dataframe format was nowhere as easy as doing it with pandas. I do believe there are some support for turning a dataframe into a dataset, but I did not find any easy way to reverse it. Futhermore, most labels in other datasets from the site I explored was typed as a ClassLabel, which holds information about how to map integers to correct label name. \n",
    "\n",
    "The process of dividing the dataset into training, validation and  test was rather smooth as they seem to rely on sklearns train_test_split split function. However, all the parts of the dataset was still contained in single DatasetDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precise-despite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:52.482329Z",
     "start_time": "2022-04-26T22:10:50.137062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silje\\miniconda3\\envs\\hug\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#collapse \n",
    "import transformers\n",
    "from datasets import load_dataset, Features, Value, ClassLabel, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graduate-theme",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:52.498336Z",
     "start_time": "2022-04-26T22:10:52.483330Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "transformers.logging.set_verbosity_warning() # Silence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handy-quilt",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.036466Z",
     "start_time": "2022-04-26T22:10:52.499335Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f31bc029a11dc270\n",
      "Reusing dataset csv (C:\\Users\\silje\\.cache\\huggingface\\datasets\\csv\\default-f31bc029a11dc270\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 99.98it/s]\n"
     ]
    }
   ],
   "source": [
    "airline_dataset = load_dataset(\"csv\", data_files=r\"data_tweets_airline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caroline-terror",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.052470Z",
     "start_time": "2022-04-26T22:10:53.037467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone'],\n",
       "        num_rows: 14640\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gross-button",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.068474Z",
     "start_time": "2022-04-26T22:10:53.053470Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "airline_dataset = airline_dataset.rename_column(\"airline_sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conventional-waste",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.100481Z",
     "start_time": "2022-04-26T22:10:53.069474Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "columns_to_remove = ['tweet_id', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count',  'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone']\n",
    "for col in columns_to_remove:\n",
    "    airline_dataset = airline_dataset.remove_columns(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developed-cache",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.116484Z",
     "start_time": "2022-04-26T22:10:53.101481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 14640\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suspended-great",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.132488Z",
     "start_time": "2022-04-26T22:10:53.117484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'neutral', 'text': '@VirginAmerica What @dhepburn said.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "asian-provincial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.148491Z",
     "start_time": "2022-04-26T22:10:53.133488Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "features = airline_dataset[\"train\"].features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conscious-roberts",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.164494Z",
     "start_time": "2022-04-26T22:10:53.150492Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Take the text and creates placeholder for username and link. \n",
    "This was a method that the model creaters suggested.\n",
    "'''\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    word = \" \".join(new_text)   \n",
    "    return word\n",
    "\n",
    "def adjust_preprocess(batch):\n",
    "    all_texts = []\n",
    "    for text in batch[\"text\"]:\n",
    "        pre = preprocess(text)\n",
    "        all_texts.append(pre)\n",
    "    batch[\"text\"] = all_texts\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elementary-local",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.180498Z",
     "start_time": "2022-04-26T22:10:53.165495Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\silje\\.cache\\huggingface\\datasets\\csv\\default-f31bc029a11dc270\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-b4164b8f269460ee.arrow\n"
     ]
    }
   ],
   "source": [
    "airline_dataset[\"train\"] = airline_dataset[\"train\"].map(adjust_preprocess, batched=True, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "norman-advisory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.196502Z",
     "start_time": "2022-04-26T22:10:53.181498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'neutral', 'text': '@user What @user said.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "internal-mongolia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.212506Z",
     "start_time": "2022-04-26T22:10:53.198502Z"
    }
   },
   "outputs": [],
   "source": [
    "features[\"label\"] = ClassLabel(names=[\"negative\", \"neutral\", \"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "empirical-sleep",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.228509Z",
     "start_time": "2022-04-26T22:10:53.213505Z"
    }
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"negative\" : 0,\n",
    "    \"neutral\" : 1,\n",
    "    \"positive\" : 2\n",
    "}\n",
    "\n",
    "def adjust_labels(batch):\n",
    "    batch[\"label\"] = [label_dict[sentiment] for sentiment in batch[\"label\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spatial-render",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.244512Z",
     "start_time": "2022-04-26T22:10:53.229509Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\silje\\.cache\\huggingface\\datasets\\csv\\default-f31bc029a11dc270\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-2a82fa99d330842d.arrow\n"
     ]
    }
   ],
   "source": [
    "airline_dataset[\"train\"] = airline_dataset[\"train\"].map(adjust_labels, batched=True, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "recovered-salmon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.260517Z",
     "start_time": "2022-04-26T22:10:53.245513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "infrared-communications",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.340535Z",
     "start_time": "2022-04-26T22:10:53.261517Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "airline_dataset = airline_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "airline_dataset_clean = airline_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "airline_dataset_clean[\"validation\"] = airline_dataset_clean.pop(\"test\")\n",
    "airline_dataset_clean[\"test\"] = airline_dataset[\"test\"]\n",
    "airline_dataset = airline_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stuck-subscriber",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:53.356538Z",
     "start_time": "2022-04-26T22:10:53.341535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 9369\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 2343\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 2928\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-network",
   "metadata": {},
   "source": [
    "# Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "neural-testimony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:54.309420Z",
     "start_time": "2022-04-26T22:10:53.357538Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "loaded-scholarship",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:54.325423Z",
     "start_time": "2022-04-26T22:10:54.310420Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eastern-growing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:56.413650Z",
     "start_time": "2022-04-26T22:10:54.326424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "blind-reach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:10:56.429654Z",
     "start_time": "2022-04-26T22:10:56.414651Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "# model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-mistress",
   "metadata": {},
   "source": [
    "# Fine-tune model \n",
    "\n",
    "It would have been possible to fine-tune the model with a custom training loop created with pytorch, but I decided to use their Training API instead.\n",
    "\n",
    "\n",
    "As with other NLP models the text has to be tokenized before given to a model. Since I was fine-tuning the model I could use the AutoTokenizer to get the proper tokenizer class in accordance to the pre-trained model. Which would be some sort of a subword tokenizer, which is what the library argues for.\n",
    "\n",
    "\n",
    "A major headache was discovering why the model refused to give any indication of accuracy, both while training and afterwards. Even with a custom function to calculate metrics, nothing would show. I think I used the better part of a day trying different approaches before realizing it was related to the dataset having three labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bacterial-swaziland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:01.576077Z",
     "start_time": "2022-04-26T22:10:56.430654Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bigger-decimal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:02.407264Z",
     "start_time": "2022-04-26T22:11:01.577077Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.57ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 30.30ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 23.99ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"],  padding=True)\n",
    "\n",
    "tokenized_datasets = airline_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "comparative-literacy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:02.423268Z",
     "start_time": "2022-04-26T22:11:02.408265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9369\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2343\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2928\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "opening-medicaid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:02.439272Z",
     "start_time": "2022-04-26T22:11:02.424269Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sharing-magnitude",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:02.455275Z",
     "start_time": "2022-04-26T22:11:02.440271Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "electrical-thought",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:02.471279Z",
     "start_time": "2022-04-26T22:11:02.458276Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "prepared-reggae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:02.534276Z",
     "start_time": "2022-04-26T22:11:02.472278Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    r\"models/hug_twitter_airline\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "extra-oriental",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:11:02.790335Z",
     "start_time": "2022-04-26T22:11:02.535277Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ancient-report",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:33:24.325081Z",
     "start_time": "2022-04-26T22:11:02.791335Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\silje\\miniconda3\\envs\\hug\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9369\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11720' max='11720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11720/11720 22:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.480900</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.842083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.607548</td>\n",
       "      <td>0.849765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.703529</td>\n",
       "      <td>0.855314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.799040</td>\n",
       "      <td>0.854033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.868735</td>\n",
       "      <td>0.848912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>1.013278</td>\n",
       "      <td>0.855741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>1.048996</td>\n",
       "      <td>0.856594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>1.099649</td>\n",
       "      <td>0.855314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>1.134570</td>\n",
       "      <td>0.857021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.176464</td>\n",
       "      <td>0.860009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-1172\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-1172\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-1172\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-1172\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-1172\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-2344\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-2344\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-2344\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-2344\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-2344\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-3516\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-3516\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-3516\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-3516\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-3516\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-4688\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-4688\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-4688\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-4688\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-4688\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-5860\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-5860\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-5860\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-5860\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-5860\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-7032\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-7032\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-7032\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-7032\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-7032\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-8204\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-8204\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-8204\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-8204\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-8204\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-9376\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-9376\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-9376\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-9376\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-9376\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-10548\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-10548\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-10548\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-10548\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-10548\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hug_twitter_airline\\checkpoint-11720\n",
      "Configuration saved in models/hug_twitter_airline\\checkpoint-11720\\config.json\n",
      "Model weights saved in models/hug_twitter_airline\\checkpoint-11720\\pytorch_model.bin\n",
      "tokenizer config file saved in models/hug_twitter_airline\\checkpoint-11720\\tokenizer_config.json\n",
      "Special tokens file saved in models/hug_twitter_airline\\checkpoint-11720\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11720, training_loss=0.1764612882210533, metrics={'train_runtime': 1341.5117, 'train_samples_per_second': 69.839, 'train_steps_per_second': 8.736, 'total_flos': 5025201623506890.0, 'train_loss': 0.1764612882210533, 'epoch': 10.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "interested-celebrity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T23:40:20.245893Z",
     "start_time": "2022-04-26T23:40:10.921793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2343\n",
      "  Batch size = 8\n",
      "Using the latest cached version of the module from C:\\Users\\silje\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Tue Apr 26 19:27:52 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1764642000198364,\n",
       " 'eval_accuracy': 0.860008536064874,\n",
       " 'eval_runtime': 9.3111,\n",
       " 'eval_samples_per_second': 251.635,\n",
       " 'eval_steps_per_second': 31.468,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "swedish-retirement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T23:40:22.115314Z",
     "start_time": "2022-04-26T23:40:21.144095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/\n",
      "Configuration saved in models/config.json\n",
      "Model weights saved in models/pytorch_model.bin\n",
      "tokenizer config file saved in models/tokenizer_config.json\n",
      "Special tokens file saved in models/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-harmony",
   "metadata": {},
   "source": [
    "# Evaluate  \n",
    "\n",
    "So in the end the model had an accuracy of 0.860008536064874 on the validation data, and 0.8562158469945356 on test data. Which is better result I got on the model fine-tuned with FastAi. Which supports the idea that transformers is next step for NLPs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "circular-person",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T22:33:31.954716Z",
     "start_time": "2022-04-26T22:33:24.326081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2928\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='952' max='366' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [366/366 1:06:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8562158469945356}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric = load_metric(\"accuracy\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug",
   "language": "python",
   "name": "hug"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
