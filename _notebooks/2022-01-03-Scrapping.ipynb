{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "authentic-chemical",
   "metadata": {},
   "source": [
    "# Data gathering \n",
    "\n",
    "As a part of this project, I wanted to construct my own data set. At first, I wrote my own scrapper with a combination of selenium and Beautiful Soup. While I was able to do some data gathering with that approach, my lack of familiarity with twitter made for some wacky code. Also, the process was quite slow. In my research, I saw that many used the Tweepy library. However, one has to have a developer account, and after a week or so and still no access I decided to try something else. Which was how I found the twitter_scraper_selenium library. Which gave the option to either scrap a specific profile for tweets or use a keyword search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-influence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T11:05:13.734793Z",
     "start_time": "2022-04-25T11:05:13.247305Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse\n",
    "from twitter_scraper_selenium import scrap_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-leader",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T11:05:15.627640Z",
     "start_time": "2022-04-25T11:05:15.623640Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse\n",
    "download_path = r\"../datasets/twitterdata\"\n",
    "\n",
    "'''\n",
    "Time recordings:\n",
    "\n",
    "10 -           21s\n",
    "100 -       1m 54s\n",
    "1000 -     15m 38s \n",
    "2000 -     15m \n",
    "5000 -  1h 15m 1s \n",
    "10000 - 2h 34m 43s \n",
    "10000 - 2h 12m 1s \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-citizenship",
   "metadata": {},
   "source": [
    "The idea behind the \"france\" dataset was that the French election would most likely promote some polarizing tweets. While working on it I was reminded that twitter is multilingual. While having gathered close to 10000 tweets only 2000 or so was English. As I should I predicted most of them were in French. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_keyword(\n",
    "    keyword=\"france\", \n",
    "    browser=\"firefox\",    \n",
    "    tweets_count=10000, \n",
    "    output_format=\"csv\",\n",
    "    filename=\"france_10000\",\n",
    "    directory=download_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-button",
   "metadata": {},
   "source": [
    "The idea behind this dataset was that the new batman film would make it a much talked about topic. However, I decided to instead use the film studio dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-cleanup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T09:00:37.349566Z",
     "start_time": "2022-04-25T06:48:36.755718Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "'''\n",
    "scrap_keyword(\n",
    "    keyword=\"batman\", \n",
    "    browser=\"firefox\",    \n",
    "    tweets_count=10000, \n",
    "    output_format=\"csv\",\n",
    "    filename=\"batman_10000\",\n",
    "    directory=download_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-floor",
   "metadata": {},
   "source": [
    "I tried to come up with an alternative to single large movies, and decided film studios might work. So I search for some of the largest I had heard about and tried to use their names as keywords. However, it is worth to mention that several of these overlap. Disney for example own both Pixar and Marvel. So the possibility of duplicated should be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-commitment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T12:08:56.440106Z",
     "start_time": "2022-04-25T11:05:18.896565Z"
    }
   },
   "outputs": [],
   "source": [
    "studios = [\"pixar\", \"dc\", \"WarnerBros.\", \"SonyPictures\", \"studioGhibli\"] #pixar   \n",
    "for studio in studios:   \n",
    "    scrap_keyword(\n",
    "        keyword= f\"{studio}\", \n",
    "        browser=\"firefox\",\n",
    "        tweets_count=2000, \n",
    "        output_format=\"csv\",\n",
    "        filename=f\"{studio}_5_2000\",\n",
    "        directory=download_path + r\"/films\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-dance",
   "metadata": {},
   "source": [
    "I though to try and recreate the airline dataset I used for both fastai and huggingface. However, the names of the airlines are rather ambiguous. So the dataset ended up looking rather faulty, so I discarded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-thomson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T13:19:16.084585Z",
     "start_time": "2022-04-25T12:08:56.554132Z"
    }
   },
   "outputs": [],
   "source": [
    "airlines = [\"United\", \"USAirways\", \"American\", \"Southwest\", \"Delta\", \"VirginAmerica\"]\n",
    "for airline in airlines:\n",
    "    scrap_keyword(keyword=airline, \n",
    "                  browser=\"firefox\", \n",
    "                  tweets_count=1000, \n",
    "                  output_format=\"csv\", \n",
    "                  filename=f\"{airline}_6_1000\",\n",
    "                  directory=download_path + r\"/airlines\"\n",
    "                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "scrap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
