{
  
    
        "post0": {
            "title": "Film studio dataset",
            "content": "import os import numpy as np import pandas as pd import glob . . paths=[ (&quot;Dreamworks&quot;, r&quot;.. datasets films DreamWorks_5_2000.csv&quot;), (&quot;Disney&quot;, r&quot;.. datasets films disney_5_2000.csv&quot;), (&quot;Marvel&quot;, r&quot;.. datasets films marvel_5_2000.csv&quot;), (&quot;Studio Ghibli&quot;, r&quot;.. datasets films studioGhibli_5_2000.csv&quot;), (&quot;Sony Pictures&quot;, r&quot;.. datasets films SonyPictures_5_2000.csv&quot;), (&quot;Warner Bros&quot;, r&quot;.. datasets films WarnerBros._5_2000.csv&quot;), (&quot;DC&quot;, r&quot;.. datasets films dc_5_2000.csv&quot;), (&quot;Pixar&quot;, r&quot;.. datasets films pixar_5_2000.csv&quot;), (&quot;Lionsgate&quot;, r&quot;.. datasets films Lionsgate_5_2000.csv&quot;) ] li = [] for studio, path in paths: df = pd.read_csv(path, index_col=0, header=0) df[&quot;studio&quot;] = studio li.append(df) df_studios = pd.concat(li, axis=0, ignore_index=True) . . df_studios.sample(5) . username name profile_picture replies retweets likes is_retweet posted_time content hashtags mentions images videos tweet_url link studio . 2282 DisneyHotelBot | „Éá„Ç£„Ç∫„Éã„Éº„Éõ„ÉÜ„É´Á©∫ÂÆ§Áä∂Ê≥ÅÔºÅ | https://twitter.com/DisneyHotelBot/photo | 0 | 0 | 0 | False | 2022-04-24T23:28:07+00:00 | Êù±‰∫¨„Éá„Ç£„Ç∫„Éã„Éº„Ç∑„Éº„Éª„Éõ„ÉÜ„É´„Éü„É©„Ç≥„Çπ„Çø„ÅÆ n‰ª•‰∏ãÊó•Á®ã„ÅåÁ©∫„ÅÑ„Å¶„ÅÑ„Åæ„ÅôÔºÅpart2 n n2022... | [&#39;„ÉÅ„Ç±„ÉÉ„Éà&#39;, &#39;Disney&#39;, &#39;„Éá„Ç£„Ç∫„Éã„Éº&#39;] | [] | [] | [] | https://twitter.com/DisneyHotelBot/status/1518... | https://t.co/HtmjkaoHYp | Disney | . 6135 themanoftomorr7 | themanoftomorrow | https://twitter.com/themanoftomorr7/photo | 0 | 1 | 0 | False | 2022-04-24T15:11:31+00:00 | Dylan O&#39;Brien is nightwing!! #Nightwing n@DCC... | [&#39;Nightwing&#39;] | [&#39;DCComics&#39;, &#39;warnerbros&#39;, &#39;wbpictures&#39;, &#39;wbd&#39;... | [&#39;https://pbs.twimg.com/media/FRHk2KdXsAAo2Iy?... | [] | https://twitter.com/themanoftomorr7/status/151... | NaN | Warner Bros | . 6248 DCtamilFC | ùêÉùêÇ ùêìùêöùê¶ùê¢ùê• ùêÖùêÇ | https://twitter.com/DCtamilFC/photo | 2 | 21 | 51 | False | 2022-04-24T12:26:02+00:00 | Warner Bros. Pictures&#39; presentation is on 4pm... | [&#39;TheFlash&#39;, &#39;BlackAdam&#39;] | [&#39;CinemaCon&#39;] | [&#39;https://pbs.twimg.com/media/FRG_CzxaIAAAdDY?... | [] | https://twitter.com/DCtamilFC/status/151820461... | NaN | Warner Bros | . 2271 ma_san100 | „Åæ„ÅÅ„Åï„Çì„Äå100„ÅÆÂÜíÈô∫„Äç | https://twitter.com/ma_san100/photo | 0 | 0 | 0 | False | 2022-04-24T23:28:16+00:00 | „Ç¶„Ç®„Çπ„Éà„Éù„Éº„ÉÅ„ÄÅ‰∏ÄÁõÆÊÉö„Çå n n#„Å≤„Å®„Çä„Éá„Ç£„Ç∫„Éã„Éº „ÅÆ„Åä‰æõ„Å´„Åó„Åü„ÅÑ n n#Â§©ÂõΩ„Å´ÊåÅ„Å£„Å¶Ë°å„Åè... | [&#39;„Å≤„Å®„Çä„Éá„Ç£„Ç∫„Éã„Éº&#39;, &#39;Â§©ÂõΩ„Å´ÊåÅ„Å£„Å¶Ë°å„ÅèÊÄù„ÅÑÂá∫&#39;, &#39;„Ç∑„Éß„ÉÉ„Éó„Éá„Ç£„Ç∫„Éã„Éº&#39;, &#39;„Éá„Ç£„Ç∫„Éã... | [] | [] | [] | https://twitter.com/ma_san100/status/151837127... | https://t.co/3I7umAwjnv | Disney | . 4106 sherwin_lui | Blastpoke | https://twitter.com/sherwin_lui/photo | 0 | 0 | 0 | False | 2022-04-24T22:50:54+00:00 | Any char walking across the main room&amp; opp is ... | [] | [] | [] | [&#39;https://video.twimg.com/tweet_video/FRI-21gX... | https://twitter.com/sherwin_lui/status/1518361... | NaN | Marvel | . df_studios.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 8410 entries, 0 to 8409 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 username 8410 non-null object 1 name 8336 non-null object 2 profile_picture 8410 non-null object 3 replies 8410 non-null int64 4 retweets 8410 non-null int64 5 likes 8410 non-null int64 6 is_retweet 8410 non-null bool 7 posted_time 8410 non-null object 8 content 8216 non-null object 9 hashtags 8410 non-null object 10 mentions 8410 non-null object 11 images 8410 non-null object 12 videos 8410 non-null object 13 tweet_url 8410 non-null object 14 link 749 non-null object 15 studio 8410 non-null object dtypes: bool(1), int64(3), object(12) memory usage: 993.9+ KB . Tried to download 2000 tweets related to the keywords Dreamworks, Disney, Marvel, Studio Ghibli, Sony Pictures, Warner Bros, DC, Pixar and Lionsgate, which are all related to some film studio. As seen above the dataset consist of only 8410 data points. . The value counts in the cell below show how the data is distributed across the studios. Which shows that some keywords worked well and others did not really work at all. . df[&quot;studio&quot;].value_counts() . Lionsgate 169 Name: studio, dtype: int64 . I chose to reduce the dataset to only keep the content and studio columns. . df = df_studios[[&quot;content&quot;, &quot;studio&quot;]].copy() . . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 8410 entries, 0 to 8409 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 content 8216 non-null object 1 studio 8410 non-null object dtypes: object(2) memory usage: 131.5+ KB . Dropping possible Nan and duplicates. . df.drop_duplicates(inplace=True) df.apply(lambda x: pd.Series(x.dropna().values)) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 8083 entries, 0 to 8409 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 content 8074 non-null object 1 studio 8083 non-null object dtypes: object(2) memory usage: 189.4+ KB . df.sample(10) . content studio . 6817 Turning Red - Estranhamente Vermelho no TV Tim... | Pixar | . 2189 Disneyland but not Disney World. | Disney | . 5681 @SonyPictures n @TomHolland1996 n @Zendaya n @... | Sony Pictures | . 7781 #pixar | Pixar | . 3847 Eu gosto de alguns filmes da Marvel, mas acho ... | Marvel | . 1086 dps de heartstopper eu to esperando ansiosamen... | Disney | . 6237 The people: Warner Bros, Bring Johnny Depp bac... | Warner Bros | . 4080 acabei de ver um tweet dc vs marvel e lembrei ... | Marvel | . 5538 i wanna go to the studio ghibli exhibit so bad... | Studio Ghibli | . 5536 my beloved friends as iconic studio ghibli sce... | Studio Ghibli | . Detect language . As I was rather awkwardly reminded when working on the &quot;France&quot; dataset, Twitter is a multilingual platform. So for the next part, I will work on detecting which language a tweet is written in. While creating a dataset with multiple languages could be interesting, it is likely to be rather skewed distributed. . At first, I tried to use TextBlob to detect the language, but it seemed the library had some issues. So I tried to use the langdetect library which said it supports 55 languages. However, trying to detect the language of the tweets directly led to some errors. Mainly an Exception saying &quot;No features in text&quot;, which was not something I had stumbled upon in the &quot;France&quot; dataset. . I tried to do some preprocessing of the text to make the language detection work. First I removed tweets with less than 5 whitespaces. Then I realized that trying to detect the language of anything which does not contain any alphabet did not work, so I tried to remove those. . df = df[df[&quot;content&quot;].str.split().str.len().ge(5)] df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 7315 entries, 0 to 8408 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 content 7315 non-null object 1 studio 7315 non-null object dtypes: object(2) memory usage: 171.4+ KB . import re . . def contains_alphabets(s): return bool(re.match(&quot;^(?=.*[a-zA-Z])&quot;, s)) df[&quot;contains_alphabets&quot;] = df[&quot;content&quot;].apply(lambda x: contains_alphabets(x)) df[&quot;contains_alphabets&quot;].value_counts() . True 7231 False 84 Name: contains_alphabets, dtype: int64 . df = df[df.contains_alphabets] . . However, I still got the same exception. So I tried some other possible solutions: . remove punctuation | remove emoji | remove html | remove urls | . import string . . def remove_punctuation(text): text = text.strip() words = text.split() table = str.maketrans(&quot;&quot;, &quot;&quot;, string.punctuation) words = [word.translate(table) for word in words] # removes Punctuation text = &quot; &quot;.join(words) return text def remove_html(text): html=re.compile(r&quot;&lt;.*?&gt;&quot;) return html.sub(r&quot;&quot;,text) def remove_url(text): url = re.compile(r&quot;https?:// S+|www . S+&quot;) return url.sub(r&quot;&quot;,text) def remove_emoji(string): emoji_pattern = re.compile(&quot;[&quot; u&quot; U0001F600- U0001F64F&quot; # emoticons u&quot; U0001F300- U0001F5FF&quot; # symbols &amp; pictographs u&quot; U0001F680- U0001F6FF&quot; # transport &amp; map symbols u&quot; U0001F1E0- U0001F1FF&quot; # flags (iOS) u&quot; U00002500- U00002BEF&quot; # chinese char u&quot; U00002702- U000027B0&quot; u&quot; U00002702- U000027B0&quot; u&quot; U000024C2- U0001F251&quot; u&quot; U0001f926- U0001f937&quot; u&quot; U00010000- U0010ffff&quot; u&quot; u2640- u2642&quot; u&quot; u2600- u2B55&quot; u&quot; u200d&quot; u&quot; u23cf&quot; u&quot; u23e9&quot; u&quot; u231a&quot; u&quot; ufe0f&quot; # dingbats u&quot; u3030&quot; &quot;]+&quot;, flags=re.UNICODE) return emoji_pattern.sub(r&quot;&quot;, string) . . df[&quot;pre_text&quot;] = df[&quot;content&quot;].apply(lambda x: remove_emoji(x)) df[&quot;pre_text&quot;] = df[&quot;pre_text&quot;].apply(lambda x: remove_url(x)) df[&quot;pre_text&quot;] = df[&quot;pre_text&quot;].apply(lambda x: remove_html(x)) df[&quot;pre_text&quot;] = df[&quot;pre_text&quot;].apply(lambda x: remove_punctuation(x)) . But even after having tried those methods, the exception prevailed. So I decided to use the unidecode library to force it to be ASCII. While not being ideal, made the exception go away. Which finally gave me some overview over the detected languages. . import unidecode . . def make_unidecode(raw_text): processed = unidecode.unidecode(raw_text) return processed . . df[&quot;pre_text&quot;] = df[&quot;pre_text&quot;].apply(lambda x: make_unidecode(x)) . df.sample(10) . content studio contains_alphabets pre_text . 2850 Lost cat Marvel in Rockford, MI US (49341) #lo... | Marvel | True | Lost cat Marvel in Rockford MI US 49341 lostcat | . 1494 can I say yes? I have high enough confidence ... | Disney | True | can I say yes I have high enough confidence to... | . 6920 Check out this listing I just added to my #Pos... | Pixar | True | Check out this listing I just added to my Posh... | . 6575 Not just you, either. nAnd the Warner Bros. ca... | Warner Bros | True | Not just you either And the Warner Bros cartoo... | . 1128 @Disney n You forgot Disney supports forced in... | Disney | True | Disney You forgot Disney supports forced indoc... | . 1975 ...regs, etc. Disney losing it&#39;s privs has vir... | Disney | True | regs etc Disney losing its privs has virtually... | . 7809 Os incr√≠veis eh o melhor filme da pixar | Pixar | True | Os incriveis eh o melhor filme da pixar | . 7805 PIXAR NO TE VOY A PERDONAR C√ìMO TRAUMES A M√ç H... | Pixar | True | PIXAR NO TE VOY A PERDONAR COMO TRAUMES A MI H... | . 5321 oh to be one of them studio ghibli girlies who... | Studio Ghibli | True | oh to be one of them studio ghibli girlies who... | . 1268 Ron DeSantis‚Äôs attack on Disney is obviously u... | Disney | True | Ron DeSantis&#39;s attack on Disney is obviously u... | . from langdetect import detect . . df[&quot;language&quot;] = df[&quot;pre_text&quot;].apply(detect) . df[&quot;language&quot;].value_counts() . en 5358 pt 670 es 571 fr 120 it 105 ca 83 id 69 de 57 af 24 et 23 so 17 nl 17 tr 16 da 14 tl 12 no 10 cy 9 ro 8 sl 8 pl 8 sv 5 hr 5 sw 5 hu 4 lt 3 cs 3 fi 3 sk 2 lv 1 vi 1 Name: language, dtype: int64 . Since I was not too happy with that solution I tried to use the googletrans library to detect a language. Which was considerably slower, using 20 minutes on something langdetect did in half a minute. Though, it worked on the unaltered text. As the value count shows, goolgetrans detects more languages. Their estimation of how many tweets were English was quite similar. I decided to only go for the rows where they agreed. . from googletrans import Translator . . def detected_language(s): detector = Translator() return detector.detect(s).lang df[&quot;language_content&quot;] = df[&quot;content&quot;].apply(lambda x: detected_language(x)) . df[&quot;language_content&quot;].value_counts() . en 5376 pt 777 es 614 fr 125 id 61 ja 59 it 40 de 28 ca 25 th 19 tr 17 tl 12 pl 10 ru 7 ar 7 nl 4 ko 4 el 4 da 3 su 3 hi 3 bn 2 fi 2 ms 2 [es, pt] 2 no 1 sk 1 si 1 [ms, id] 1 hu 1 [id, jw] 1 [es, ro] 1 [mr, hi] 1 ro 1 ig 1 fa 1 [ja, zh-CN] 1 bg 1 [pt, en] 1 [en, el] 1 af 1 gu 1 [hi, mr] 1 [en, fr] 1 [en, da] 1 cs 1 rw 1 sv 1 te 1 kn 1 Name: language_content, dtype: int64 . df_en = df.loc[(df[&quot;language&quot;].str.startswith(&quot;en&quot;)) &amp; (df[&quot;language_content&quot;].str.startswith(&quot;en&quot;))] . . df_en = df_en[[&quot;content&quot;]].copy() . . df_en.sample(10) . content . 3099 Watching No Way Home for the first time since seeing it in theaters. Still the best Marvel film to date | . 8159 2 days until I can watch Turning Red! #Disney #Pixar #TurningRed | . 6163 Gonna get a platinum trophy as part of my mid year bucket list n n#legostarwars #starwars #theforceawakens #howiplaystation #playstation #playstationau #ps4 #bucketlist #starwarstheforceawakens #warnerbros #ttgames #playstation4 | . 3268 I like marvel movies too, just not as much as others. I think true cinema is something that stays with you, not a black and white Albanian film from the pov of a worm. | . 1497 23,95‚Ç¨ Disney Classic Games Collection: The Jungle Book, Aladdin, and The Lion King - NSW https://amzn.to/3EIb11M a trav√©s de n@amazon | . 6686 @alioverse n #muchmeow 3D animated NFTs from Artists with previous works done for Disney, Pixar and Dreamworks. | . 6459 @warnerbros n @hbomax n please reconsider the Boondocks revival. | . 7605 But you can‚Äôt make a good argument that superman‚Äôs race is integral to the character. Hence we‚Äôre seeing diversity in these traditional roles, ie The Zods are black in the Krypton show and Young Justice, and they‚Äôre tossing around the idea of a black Superman. | . 2947 JRJR is still one of the VERY best Spidey artists out there (especially since he seems to be over his tiny ankles phase). Bold cinematic compositions with a dash of JRSR and Kirby still peeping out here and there. JR will always be a bright star in the comics cosmos. | . 1494 can I say yes? I have high enough confidence to say that this is a disney princess! you‚Äôre not wrong, oppa! and how about you? are you some kind of prince or something? | . df_en.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 5245 entries, 0 to 8408 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 content 5245 non-null object dtypes: object(1) memory usage: 82.0+ KB . df_en.describe() . content . count 5245 | . unique 5170 | . top no words. Make it happen. #start #change #now #justiceforjohnnydepp #johnnydepp #istandwithjohnnydepp #justicereform n@disney n @warnerbrosentertainment n@sonypictures n @paramountpics n @universalpictures @columbiapictures n@lionsgate n @dreamworks n @theacademy n @sagaftra | . freq 3 | . Label data . Since the gathered data was unlabeled I wanted to try and explore possible ways of labeling it. Most of my time was used exploring the Snorkel library. Their idea is to create several labeling functions where domain information is injected, and then automatically estimate each function&#39;s accuracy and correlation. Then their estimate is combined to determine output labels. However, this relies on having some domain knowledge and an idea of what might indicate each label. Which I do not have, nor wanted to use the time to determine. What I found was that most of their examples with labeling polarity and sentiment relied on a library named TextBlob (the same I tried to use to detect languages). So I decided to explore using that library as a possibility. . The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. . So after having extracted the sentiment from the tweet, thresholds are needed to sort them accordingly. . from textblob import TextBlob from textblob.sentiments import NaiveBayesAnalyzer . . df_en[&quot;score&quot;] = .0 . . for i, x in df_en.content.iteritems(): score = TextBlob(x) df_en.at[i, &quot;score&quot;] = score.sentiment.polarity . df_en.head() . content score . 0 Oh man, look at Dreamworks, releasing movies with really cool animation styles! This looks amazing! And Trolls 2 had great animation before it too! | 0.729167 | . 3 Don‚Äôt go past this 1/1 n@alioverse n #muchmeow 3D animated NFTs from Artists with previous works done for Disney, Pixar and Dreamworks. | -0.208333 | . 5 Why does he do the dreamworks face in every photo | 0.000000 | . 6 i can&#39;t believe jerma is the purple man from hit dreamworks movie home | 0.000000 | . 7 @alioverse n #muchmeow 3D animated NFTs from Artists with previous works done for Disney, Pixar and Dreamworks. | -0.166667 | . df_en.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 5245 entries, 0 to 8408 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 content 5245 non-null object 1 score 5245 non-null float64 dtypes: float64(1), object(1) memory usage: 252.0+ KB . def polarity_to_label(x): if(x &gt;= -1 and x &lt; 0): return &quot;negative&quot; if(x == 0): return &quot;neutral&quot; if(x &gt; 0 and x &lt;= 1): return &quot;positive&quot; df_en[&quot;label&quot;] = df_en[&quot;score&quot;].apply(lambda x: polarity_to_label(x)) . df_en[&quot;label&quot;].value_counts() . positive 2580 neutral 1699 negative 966 Name: label, dtype: int64 . df_en = df_en[[&quot;content&quot;, &quot;label&quot;]].copy() . pd.set_option(&quot;display.max_colwidth&quot;, None) . . df_en.sample(10) . content label . 2402 Disney employs 80,000 Floridians, they most certainly can make a stand to protect their civil rights n nAlso makes good business sense - they benefit from a happy pool of potential employees | positive | . 6741 ok I caught up on another Pixar and yeah that&#39;s quite a concept for the mogul&#39;s house the family use in The Incredibles 2. not clear what really holds anything up though. also looks like a nightmare for heating in winter, and A/C in summer | positive | . 4675 Maybe not in the cinemas but MARVEL HEROES are definitely in the house | neutral | . 4808 I took a walk in Koganei, where Studio Ghibli is located. It is an area with many attractions such as a nursery school for the staff&#39;s children and Hayao Miyazaki&#39;s atelier. n#studioghibli #hayaomiyazaki #totoro n nhttps://mtlnk.net/j_s%253A%252F%252Fwww.bura-bura-tv.com%252F139‚Ä¶ | positive | . 6055 And Warner bros large mint projects. They bring new customers to the marky | positive | . 8011 Also Pixar back in 1988 wasn‚Äôt as big as it is now. | neutral | . 7149 How sick would it be if Disney/Pixar did a 4*Town virtual concert residency at Walt Disney World in Florida? | negative | . 7584 by your logic we are canon to the mcu, and so is dc , and pixar, and dreamworks, and illumination... | neutral | . 3711 Lmao. Making exactly what everyone is asking for is all the time is lame af. That&#39;s why the entertainment industry utterly lacks creativity right now. To many focus groups, to much pandering to the market. n nMarvel movie after marvel movie, marvel games gonna end up much the same. | positive | . 7097 To infinity and beyond! nBlast off with our newest legos for ‚ÄúLightyear‚Äù! n n n n#buzzlightyear #lightyear #disney #pixar #toystory #space #barnesandnoble #142bn | neutral | . df_en.to_csv(r&quot;../datasets/studio_data.csv&quot;) . .",
            "url": "https://thestiana.github.io/vigilant-telegram/2022/01/04/Film-Studios.html",
            "relUrl": "/2022/01/04/Film-Studios.html",
            "date": " ‚Ä¢ Jan 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Data gathering",
            "content": "from twitter_scraper_selenium import scrap_keyword . . download_path = r&quot;../datasets/twitterdata&quot; &#39;&#39;&#39; Time recordings: 10 - 21s 100 - 1m 54s 1000 - 15m 38s 2000 - 15m 5000 - 1h 15m 1s 10000 - 2h 34m 43s 10000 - 2h 12m 1s &#39;&#39;&#39; . . The idea behind the &quot;france&quot; dataset was that the French election would most likely promote some polarizing tweets. While working on it I was reminded that twitter is multilingual. While having gathered close to 10000 tweets only 2000 or so was English. As I should I predicted most of them were in French. . scrap_keyword( keyword=&quot;france&quot;, browser=&quot;firefox&quot;, tweets_count=10000, output_format=&quot;csv&quot;, filename=&quot;france_10000&quot;, directory=download_path ) . The idea behind this dataset was that the new batman film would make it a much talked about topic. However, I decided to instead use the film studio dataset. . &#39;&#39;&#39; &#39;&#39;&#39; scrap_keyword( keyword=&quot;batman&quot;, browser=&quot;firefox&quot;, tweets_count=10000, output_format=&quot;csv&quot;, filename=&quot;batman_10000&quot;, directory=download_path ) . I tried to come up with an alternative to single large movies, and decided film studios might work. So I search for some of the largest I had heard about and tried to use their names as keywords. However, it is worth to mention that several of these overlap. Disney for example own both Pixar and Marvel. So the possibility of duplicated should be there. . studios = [&quot;pixar&quot;, &quot;dc&quot;, &quot;WarnerBros.&quot;, &quot;SonyPictures&quot;, &quot;studioGhibli&quot;] #pixar for studio in studios: scrap_keyword( keyword= f&quot;{studio}&quot;, browser=&quot;firefox&quot;, tweets_count=2000, output_format=&quot;csv&quot;, filename=f&quot;{studio}_5_2000&quot;, directory=download_path + r&quot;/films&quot; ) . I though to try and recreate the airline dataset I used for both fastai and huggingface. However, the names of the airlines are rather ambiguous. So the dataset ended up looking rather faulty, so I discarded it. . airlines = [&quot;United&quot;, &quot;USAirways&quot;, &quot;American&quot;, &quot;Southwest&quot;, &quot;Delta&quot;, &quot;VirginAmerica&quot;] for airline in airlines: scrap_keyword(keyword=airline, browser=&quot;firefox&quot;, tweets_count=1000, output_format=&quot;csv&quot;, filename=f&quot;{airline}_6_1000&quot;, directory=download_path + r&quot;/airlines&quot; ) .",
            "url": "https://thestiana.github.io/vigilant-telegram/2022/01/03/Scrapping.html",
            "relUrl": "/2022/01/03/Scrapping.html",
            "date": " ‚Ä¢ Jan 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fine-tune transformer with  Hugging Face",
            "content": "Prepare dataset . The Hugging Face library has it own dataset format which is different than the pandas dataframe, which made for a rather tricky start. . Those which made the model had also served an function which created placeholders for both username and link, which seemed sensible to apply to the tweets. However, applying a function to a feature in this dataframe format was nowhere as easy as doing it with pandas. I do believe there are some support for turning a dataframe into a dataset, but I did not find any easy way to reverse it. Futhermore, most labels in other datasets from the site I explored was typed as a ClassLabel, which holds information about how to map integers to correct label name. . The process of dividing the dataset into training, validation and test was rather smooth as they seem to rely on sklearns train_test_split split function. However, all the parts of the dataset was still contained in single DatasetDict. . import transformers from datasets import load_dataset, Features, Value, ClassLabel, load_metric . . C: Users silje miniconda3 envs hug lib site-packages tqdm auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm . transformers.logging.set_verbosity_warning() # Silence . . airline_dataset = load_dataset(&quot;csv&quot;, data_files=r&quot;data_tweets_airline.csv&quot;) . Using custom data configuration default-f31bc029a11dc270 Reusing dataset csv (C: Users silje .cache huggingface datasets csv default-f31bc029a11dc270 0.0.0 433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519) 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 99.98it/s] . airline_dataset . DatasetDict({ train: Dataset({ features: [&#39;tweet_id&#39;, &#39;airline_sentiment&#39;, &#39;airline_sentiment_confidence&#39;, &#39;negativereason&#39;, &#39;negativereason_confidence&#39;, &#39;airline&#39;, &#39;airline_sentiment_gold&#39;, &#39;name&#39;, &#39;negativereason_gold&#39;, &#39;retweet_count&#39;, &#39;text&#39;, &#39;tweet_coord&#39;, &#39;tweet_created&#39;, &#39;tweet_location&#39;, &#39;user_timezone&#39;], num_rows: 14640 }) }) . airline_dataset = airline_dataset.rename_column(&quot;airline_sentiment&quot;, &quot;label&quot;) . . columns_to_remove = [&#39;tweet_id&#39;, &#39;airline_sentiment_confidence&#39;, &#39;negativereason&#39;, &#39;negativereason_confidence&#39;, &#39;airline&#39;, &#39;airline_sentiment_gold&#39;, &#39;name&#39;, &#39;negativereason_gold&#39;, &#39;retweet_count&#39;, &#39;tweet_coord&#39;, &#39;tweet_created&#39;, &#39;tweet_location&#39;, &#39;user_timezone&#39;] for col in columns_to_remove: airline_dataset = airline_dataset.remove_columns(col) . . airline_dataset . DatasetDict({ train: Dataset({ features: [&#39;label&#39;, &#39;text&#39;], num_rows: 14640 }) }) . airline_dataset[&quot;train&quot;][0] . {&#39;label&#39;: &#39;neutral&#39;, &#39;text&#39;: &#39;@VirginAmerica What @dhepburn said.&#39;} . features = airline_dataset[&quot;train&quot;].features.copy() . . &#39;&#39;&#39; Take the text and creates placeholder for username and link. This was a method that the model creaters suggested. &#39;&#39;&#39; def preprocess(text): new_text = [] for t in text.split(&quot; &quot;): t = &#39;@user&#39; if t.startswith(&#39;@&#39;) and len(t) &gt; 1 else t t = &#39;http&#39; if t.startswith(&#39;http&#39;) else t new_text.append(t) word = &quot; &quot;.join(new_text) return word def adjust_preprocess(batch): all_texts = [] for text in batch[&quot;text&quot;]: pre = preprocess(text) all_texts.append(pre) batch[&quot;text&quot;] = all_texts return batch . airline_dataset[&quot;train&quot;] = airline_dataset[&quot;train&quot;].map(adjust_preprocess, batched=True, features=features) . Loading cached processed dataset at C: Users silje .cache huggingface datasets csv default-f31bc029a11dc270 0.0.0 433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519 cache-b4164b8f269460ee.arrow . airline_dataset[&quot;train&quot;][0] . {&#39;label&#39;: &#39;neutral&#39;, &#39;text&#39;: &#39;@user What @user said.&#39;} . features[&quot;label&quot;] = ClassLabel(names=[&quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;]) . label_dict = { &quot;negative&quot; : 0, &quot;neutral&quot; : 1, &quot;positive&quot; : 2 } def adjust_labels(batch): batch[&quot;label&quot;] = [label_dict[sentiment] for sentiment in batch[&quot;label&quot;]] return batch . airline_dataset[&quot;train&quot;] = airline_dataset[&quot;train&quot;].map(adjust_labels, batched=True, features=features) . Loading cached processed dataset at C: Users silje .cache huggingface datasets csv default-f31bc029a11dc270 0.0.0 433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519 cache-2a82fa99d330842d.arrow . airline_dataset[&quot;train&quot;].features . {&#39;label&#39;: ClassLabel(num_classes=3, names=[&#39;negative&#39;, &#39;neutral&#39;, &#39;positive&#39;], id=None), &#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)} . airline_dataset = airline_dataset[&quot;train&quot;].train_test_split(test_size=0.2) airline_dataset_clean = airline_dataset[&quot;train&quot;].train_test_split(train_size=0.8, seed=42) airline_dataset_clean[&quot;validation&quot;] = airline_dataset_clean.pop(&quot;test&quot;) airline_dataset_clean[&quot;test&quot;] = airline_dataset[&quot;test&quot;] airline_dataset = airline_dataset_clean . . airline_dataset . DatasetDict({ train: Dataset({ features: [&#39;label&#39;, &#39;text&#39;], num_rows: 9369 }) validation: Dataset({ features: [&#39;label&#39;, &#39;text&#39;], num_rows: 2343 }) test: Dataset({ features: [&#39;label&#39;, &#39;text&#39;], num_rows: 2928 }) }) . Pre-trained model . from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding from transformers import Trainer, TrainingArguments . . MODEL = f&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot; . model = AutoModelForSequenceClassification.from_pretrained(MODEL) . Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: [&#39;roberta.pooler.dense.weight&#39;, &#39;roberta.pooler.dense.bias&#39;] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). . # model.save_pretrained(MODEL) . . Fine-tune model . It would have been possible to fine-tune the model with a custom training loop created with pytorch, but I decided to use their Training API instead. . As with other NLP models the text has to be tokenized before given to a model. Since I was fine-tuning the model I could use the AutoTokenizer to get the proper tokenizer class in accordance to the pre-trained model. Which would be some sort of a subword tokenizer, which is what the library argues for. . A major headache was discovering why the model refused to give any indication of accuracy, both while training and afterwards. Even with a custom function to calculate metrics, nothing would show. I think I used the better part of a day trying different approaches before realizing it was related to the dataset having three labels. . tokenizer = AutoTokenizer.from_pretrained(MODEL) . def tokenize_function(dataset): return tokenizer(dataset[&quot;text&quot;], padding=True) tokenized_datasets = airline_dataset.map(tokenize_function, batched=True) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00&lt;00:00, 20.57ba/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00&lt;00:00, 30.30ba/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00&lt;00:00, 23.99ba/s] . tokenized_datasets . DatasetDict({ train: Dataset({ features: [&#39;label&#39;, &#39;text&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;], num_rows: 9369 }) validation: Dataset({ features: [&#39;label&#39;, &#39;text&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;], num_rows: 2343 }) test: Dataset({ features: [&#39;label&#39;, &#39;text&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;], num_rows: 2928 }) }) . import numpy as np . . def compute_metrics(eval_preds): metric = load_metric(&quot;accuracy&quot;) logits, labels = eval_preds predictions = np.argmax(logits, axis=-1) return metric.compute(predictions=predictions, references=labels) . data_collator = DataCollatorWithPadding(tokenizer=tokenizer) . training_args = TrainingArguments( r&quot;models/hug_twitter_airline&quot;, evaluation_strategy=&quot;epoch&quot;, save_strategy=&quot;epoch&quot;, num_train_epochs=10 ) . trainer = Trainer( model, training_args, train_dataset=tokenized_datasets[&quot;train&quot;], eval_dataset=tokenized_datasets[&quot;validation&quot;], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics ) . trainer.train() . The following columns in the training set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. C: Users silje miniconda3 envs hug lib site-packages transformers optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 9369 Num Epochs = 10 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 11720 . . [11720/11720 22:18, Epoch 10/10] Epoch Training Loss Validation Loss Accuracy . 1 | 0.480900 | 0.498291 | 0.842083 | . 2 | 0.371900 | 0.607548 | 0.849765 | . 3 | 0.306500 | 0.703529 | 0.855314 | . 4 | 0.205100 | 0.799040 | 0.854033 | . 5 | 0.137800 | 0.868735 | 0.848912 | . 6 | 0.100100 | 1.013278 | 0.855741 | . 7 | 0.064400 | 1.048996 | 0.856594 | . 8 | 0.057900 | 1.099649 | 0.855314 | . 9 | 0.023600 | 1.134570 | 0.857021 | . 10 | 0.022800 | 1.176464 | 0.860009 | . &lt;/div&gt; &lt;/div&gt; The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-1172 Configuration saved in models/hug_twitter_airline checkpoint-1172 config.json Model weights saved in models/hug_twitter_airline checkpoint-1172 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-1172 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-1172 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-2344 Configuration saved in models/hug_twitter_airline checkpoint-2344 config.json Model weights saved in models/hug_twitter_airline checkpoint-2344 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-2344 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-2344 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-3516 Configuration saved in models/hug_twitter_airline checkpoint-3516 config.json Model weights saved in models/hug_twitter_airline checkpoint-3516 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-3516 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-3516 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-4688 Configuration saved in models/hug_twitter_airline checkpoint-4688 config.json Model weights saved in models/hug_twitter_airline checkpoint-4688 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-4688 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-4688 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-5860 Configuration saved in models/hug_twitter_airline checkpoint-5860 config.json Model weights saved in models/hug_twitter_airline checkpoint-5860 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-5860 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-5860 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-7032 Configuration saved in models/hug_twitter_airline checkpoint-7032 config.json Model weights saved in models/hug_twitter_airline checkpoint-7032 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-7032 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-7032 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-8204 Configuration saved in models/hug_twitter_airline checkpoint-8204 config.json Model weights saved in models/hug_twitter_airline checkpoint-8204 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-8204 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-8204 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-9376 Configuration saved in models/hug_twitter_airline checkpoint-9376 config.json Model weights saved in models/hug_twitter_airline checkpoint-9376 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-9376 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-9376 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-10548 Configuration saved in models/hug_twitter_airline checkpoint-10548 config.json Model weights saved in models/hug_twitter_airline checkpoint-10548 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-10548 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-10548 special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Saving model checkpoint to models/hug_twitter_airline checkpoint-11720 Configuration saved in models/hug_twitter_airline checkpoint-11720 config.json Model weights saved in models/hug_twitter_airline checkpoint-11720 pytorch_model.bin tokenizer config file saved in models/hug_twitter_airline checkpoint-11720 tokenizer_config.json Special tokens file saved in models/hug_twitter_airline checkpoint-11720 special_tokens_map.json Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=11720, training_loss=0.1764612882210533, metrics={&#39;train_runtime&#39;: 1341.5117, &#39;train_samples_per_second&#39;: 69.839, &#39;train_steps_per_second&#39;: 8.736, &#39;total_flos&#39;: 5025201623506890.0, &#39;train_loss&#39;: 0.1764612882210533, &#39;epoch&#39;: 10.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; trainer.evaluate() . The following columns in the evaluation set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 2343 Batch size = 8 Using the latest cached version of the module from C: Users silje .cache huggingface modules datasets_modules metrics accuracy bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Tue Apr 26 19:27:52 2022) since it couldn&#39;t be found locally at accuracy, or remotely on the Hugging Face Hub. . {&#39;eval_loss&#39;: 1.1764642000198364, &#39;eval_accuracy&#39;: 0.860008536064874, &#39;eval_runtime&#39;: 9.3111, &#39;eval_samples_per_second&#39;: 251.635, &#39;eval_steps_per_second&#39;: 31.468, &#39;epoch&#39;: 10.0} . trainer.save_model(&quot;models/&quot;) . Saving model checkpoint to models/ Configuration saved in models/config.json Model weights saved in models/pytorch_model.bin tokenizer config file saved in models/tokenizer_config.json Special tokens file saved in models/special_tokens_map.json . Evaluate . So in the end the model had an accuracy of 0.860008536064874 on the validation data, and 0.8562158469945356 on test data. Which is better result I got on the model fine-tuned with FastAi. Which supports the idea that transformers is next step for NLPs. . predictions = trainer.predict(tokenized_datasets[&quot;test&quot;]) preds = np.argmax(predictions.predictions, axis=-1) metric = load_metric(&quot;accuracy&quot;) metric.compute(predictions=preds, references=predictions.label_ids) . The following columns in the test set don&#39;t have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`, you can safely ignore this message. ***** Running Prediction ***** Num examples = 2928 Batch size = 8 . . [366/366 1:06:52] {&#39;accuracy&#39;: 0.8562158469945356} . &lt;/div&gt; .",
            "url": "https://thestiana.github.io/vigilant-telegram/2022/01/02/HuggingFace.html",
            "relUrl": "/2022/01/02/HuggingFace.html",
            "date": " ‚Ä¢ Jan 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "ULMFit with FastAI",
            "content": "from fastai.text.all import * from fastai.text import * import pandas as pd import numpy as np from pathlib import Path . . Data exploration . Stared with downloading and familiarizing my self with the dataset. . df_check = pd.read_csv(r&quot;../datasets/Tweets.csv.zip&quot;, nrows=10) df_check.head() . tweet_id airline_sentiment airline_sentiment_confidence negativereason negativereason_confidence airline airline_sentiment_gold name negativereason_gold retweet_count text tweet_coord tweet_created tweet_location user_timezone . 0 570306133677760513 | neutral | 1.0000 | NaN | NaN | Virgin America | NaN | cairdin | NaN | 0 | @VirginAmerica What @dhepburn said. | NaN | 2015-02-24 11:35:52 -0800 | NaN | Eastern Time (US &amp; Canada) | . 1 570301130888122368 | positive | 0.3486 | NaN | 0.0000 | Virgin America | NaN | jnardino | NaN | 0 | @VirginAmerica plus you&#39;ve added commercials to the experience... tacky. | NaN | 2015-02-24 11:15:59 -0800 | NaN | Pacific Time (US &amp; Canada) | . 2 570301083672813571 | neutral | 0.6837 | NaN | NaN | Virgin America | NaN | yvonnalynn | NaN | 0 | @VirginAmerica I didn&#39;t today... Must mean I need to take another trip! | NaN | 2015-02-24 11:15:48 -0800 | Lets Play | Central Time (US &amp; Canada) | . 3 570301031407624196 | negative | 1.0000 | Bad Flight | 0.7033 | Virgin America | NaN | jnardino | NaN | 0 | @VirginAmerica it&#39;s really aggressive to blast obnoxious &quot;entertainment&quot; in your guests&#39; faces &amp;amp; they have little recourse | NaN | 2015-02-24 11:15:36 -0800 | NaN | Pacific Time (US &amp; Canada) | . 4 570300817074462722 | negative | 1.0000 | Can&#39;t Tell | 1.0000 | Virgin America | NaN | jnardino | NaN | 0 | @VirginAmerica and it&#39;s a really big bad thing about it | NaN | 2015-02-24 11:14:45 -0800 | NaN | Pacific Time (US &amp; Canada) | . Started with looking at a few data points to get a little overview, and can from this see that the class label is airline_sentiment which is expressed as a string (or object), the airline review is contained under the header text. . df_all = pd.read_csv(&quot;../datasets/Tweets.csv.zip&quot;) df_all.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 14640 entries, 0 to 14639 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 tweet_id 14640 non-null int64 1 airline_sentiment 14640 non-null object 2 airline_sentiment_confidence 14640 non-null float64 3 negativereason 9178 non-null object 4 negativereason_confidence 10522 non-null float64 5 airline 14640 non-null object 6 airline_sentiment_gold 40 non-null object 7 name 14640 non-null object 8 negativereason_gold 32 non-null object 9 retweet_count 14640 non-null int64 10 text 14640 non-null object 11 tweet_coord 1019 non-null object 12 tweet_created 14640 non-null object 13 tweet_location 9907 non-null object 14 user_timezone 9820 non-null object dtypes: float64(2), int64(2), object(11) memory usage: 1.7+ MB . # Wanted to make sure that none of the sentiments were nan. df_all[&quot;airline_sentiment&quot;].isna().sum() . . 0 . df_all[&quot;airline_sentiment&quot;].value_counts() . negative 9178 neutral 3099 positive 2363 Name: airline_sentiment, dtype: int64 . The dataset has 14640 entries, and neither airline_sentiment nor text seems to lack any. However, it is splint into three different classes negative, neutral and positive. Something noteworthy is that the dataset contains lots more negative reviews. As such, it could be interesting to see if the dataset has other skewed distributions and try to see how represented the different airlines are. . df_all[&quot;airline&quot;].value_counts() . United 3822 US Airways 2913 American 2759 Southwest 2420 Delta 2222 Virgin America 504 Name: airline, dtype: int64 . Only kept the text and sentiment column, and also renamed it to label. . df = df_all[[&quot;airline_sentiment&quot;,&quot;text&quot;]].copy() df.rename(columns={&quot;airline_sentiment&quot;:&quot;label&quot;}, inplace=True) . An is_valid column signals to the DataBlock which parts of the dataset is validation and which is training. So created 20% validation data. . np.random.seed(2020) df[&quot;is_valid&quot;]=np.random.choice([True, False], len(df), p=[0.8,0.2 ]) . Language Model . Tokenization and numericalization of the dataset is something which must be considered. Tokenization being the part where the corpus is broken down to tokens, which are used to represent the text. Furthermore, since models rely on numerical data, the tokens has to be given an number. Passing a TextBlock to a Datablock in FastAi handels both the tokenization and numericalization of the data. DataBlock is then used to create the DataLoader. Then fastai&#39;s pretrained language model is fine-tuned on the Twitter dataset. . tweet_lm = DataBlock(blocks=TextBlock.from_df(&quot;text&quot;, is_lm=True), get_x=ColReader(&quot;text&quot;), splitter=ColSplitter()) . tweet_lm = tweet_lm.dataloaders(df, bs=16, seq_len=72) . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `n_workers` has to be changed to 0 to avoid getting stuck . tweet_lm.show_batch(max_n=5) . text text_ . 0 xxbos @jetblue xxmaj spent most of the winter in xxup xxunk with xxmaj mom , jetblue will take me home to xxup fl via xxup bos on xxmaj mon . xxmaj miss hubby , xxunk , warm # xxunk xxbos @southwestair want to explain why i was on hold for 2 + hrs tonight trying to reach customer service only to learn they &#39;re only there mon - fr ? xxbos @united | @jetblue xxmaj spent most of the winter in xxup xxunk with xxmaj mom , jetblue will take me home to xxup fl via xxup bos on xxmaj mon . xxmaj miss hubby , xxunk , warm # xxunk xxbos @southwestair want to explain why i was on hold for 2 + hrs tonight trying to reach customer service only to learn they &#39;re only there mon - fr ? xxbos @united xxup | . 1 when i made a xxmaj united weather change , free / 5 min . xxmaj you guys 2 hrs on hold &amp; &amp; $ 25 . xxbos @americanair ; @usairways xxup us 728 / xxmaj feb 21 . xxmaj unprofessional , xxunk , xxunk , xxunk communication , and xxunk solutions . xxbos @jetblue i would drive 2 1 / 2 hours to jfk to board jetblue to xxmaj vegas instead of | i made a xxmaj united weather change , free / 5 min . xxmaj you guys 2 hrs on hold &amp; &amp; $ 25 . xxbos @americanair ; @usairways xxup us 728 / xxmaj feb 21 . xxmaj unprofessional , xxunk , xxunk , xxunk communication , and xxunk solutions . xxbos @jetblue i would drive 2 1 / 2 hours to jfk to board jetblue to xxmaj vegas instead of closer | . 2 xxbos @southwestair are xxunk that the xxunk &quot; , xxunk xxunk free or xxunk are automatically $ 75 / each way ? xxmaj the xxup xxunk page is n&#39;t very clear to me . xxbos @united it was delivered ! xxmaj thank you for making sure it arrived at my xxunk ! xxbos @americanair xxunk on the terminal , what &#39; the best option ? xxmaj arrive xxup c , depart xxup | @southwestair are xxunk that the xxunk &quot; , xxunk xxunk free or xxunk are automatically $ 75 / each way ? xxmaj the xxup xxunk page is n&#39;t very clear to me . xxbos @united it was delivered ! xxmaj thank you for making sure it arrived at my xxunk ! xxbos @americanair xxunk on the terminal , what &#39; the best option ? xxmaj arrive xxup c , depart xxup xxunk | . 3 tv &#39;s xxunk xxmaj xxunk xxmaj xxunk . xxmaj tough to xxunk that quickly . xxmaj not a complaint , just an xxunk . xxbos @jetblue loved the service from the staff at xxmaj newark today . n n xxmaj good service goes along way . n n i appreciate your xxunk n n xxmaj nj ‚úà xxunk n n‚ñÅ xxrep 3 xxunk xxbos @virginamerica xxmaj well , i xxunk ‚Ä¶ but xxup now i xxup | &#39;s xxunk xxmaj xxunk xxmaj xxunk . xxmaj tough to xxunk that quickly . xxmaj not a complaint , just an xxunk . xxbos @jetblue loved the service from the staff at xxmaj newark today . n n xxmaj good service goes along way . n n i appreciate your xxunk n n xxmaj nj ‚úà xxunk n n‚ñÅ xxrep 3 xxunk xxbos @virginamerica xxmaj well , i xxunk ‚Ä¶ but xxup now i xxup do | . 4 a call back feature ! xxbos ‚Äú @jetblue : xxmaj our fleet &#39;s on fleek . http : / / t.co / xxunk ‚Äù + lol wow xxbos @united xxmaj i &#39;m not know if the seats are actually xxunk than other seats but they feel like it . xxmaj or maybe xxmaj i &#39;m extra xxunk . xxbos @united you &#39;d learn if you listen to your customers ‚Ä¶ you do | call back feature ! xxbos ‚Äú @jetblue : xxmaj our fleet &#39;s on fleek . http : / / t.co / xxunk ‚Äù + lol wow xxbos @united xxmaj i &#39;m not know if the seats are actually xxunk than other seats but they feel like it . xxmaj or maybe xxmaj i &#39;m extra xxunk . xxbos @united you &#39;d learn if you listen to your customers ‚Ä¶ you do want | . learn_lm = language_model_learner(tweet_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . learn_lm.fit_one_cycle(10, 2e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 6.161669 | 5.874311 | 0.159417 | 355.779419 | 00:15 | . 1 | 5.595076 | 5.006215 | 0.192024 | 149.338440 | 00:11 | . 2 | 4.914672 | 4.457724 | 0.215060 | 86.290855 | 00:11 | . 3 | 4.390341 | 4.201617 | 0.240306 | 66.794266 | 00:11 | . 4 | 4.096628 | 4.130862 | 0.247976 | 62.231560 | 00:12 | . 5 | 3.926286 | 4.094766 | 0.251335 | 60.025272 | 00:11 | . 6 | 3.816551 | 4.074054 | 0.253584 | 58.794819 | 00:12 | . 7 | 3.750387 | 4.064131 | 0.254351 | 58.214314 | 00:11 | . 8 | 3.703751 | 4.060599 | 0.254894 | 58.009068 | 00:11 | . 9 | 3.677141 | 4.060071 | 0.254925 | 57.978401 | 00:11 | . learn_lm.save_encoder(&quot;finetuned&quot;) . . #learn_lm.save(&#39;10epoch_lm&#39;) . . Classification . A DataBlock is created for the classification fine-turning. Here the vocab created for the language model is passed along, which makes the token match the index, and lets the models work together. Then a DataLoader is created. As with above, the batch shows that the text is tokenized. An text_classifier_learner crated and the previously saved encoder is loaded in. When training the classification model FastAi suggests gradual unfreezing the model and discriminative learning rates. I first tried without that approach and got an accuracy of 76.98% accuracy, which was worse than the results shown bellow of an accuracy of 78.97% accuracy. The confusion matrix shows that the model is really good at predicting negative tweets, but lack the same understanding of neutral and positive. As such, 829 of the tweets label neutral was labeled negative. The relative small dataset, and the skewed distribution of labels might be some reasons. . tweet_clas = DataBlock(blocks=(TextBlock.from_df(&quot;text&quot;, seq_len=72, vocab=tweet_lm.vocab), CategoryBlock), get_x=ColReader(&quot;text&quot;), get_y=ColReader(&quot;label&quot;), splitter=ColSplitter()) . tweet_clas = tweet_clas.dataloaders(df, bs=16) . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `n_workers` has to be changed to 0 to avoid getting stuck Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `number_workers` is changed to 0 to avoid getting stuck . tweet_clas.show_batch(max_n=5) . text category . 0 xxbos @usairways xxup plans xxup changed ! xxup is xxup that xxup what u xxup call xxup not xxup xxunk xxup able 2 xxup park xxup on xxup time xxup planes w a 2 xxup hr xxup layover n b xxup able 2 xxup make xxup ur xxup connection ! # seriously | negative | . 1 xxbos @usairways e xxrep 4 y ! xxmaj cancelled xxmaj flightlations , xxmaj flight xxmaj booking xxmaj problemss , reflight xxmaj booking xxmaj problemss , but y&#39; all got me on the same flight out tonight ( not tomorrow ) &amp; &amp; the xxup fc upgrade . xxmaj thx ! | positive | . 2 xxbos @united xxup fail xxmaj you xxmaj cancelled xxmaj flightled our flight frm xxup xxunk and then used our reserv home to xxup iah ( from xxup sea ) for reflight xxmaj booking xxmaj problems w / out xxup our xxup xxunk ! ! # xxup xxunk | negative | . 3 xxbos @united : xxmaj unhappy with xxmaj united &#39;s service ? n n xxmaj read xxmaj xxunk xxmaj xxunk &#39;s open letter to ual &#39;s xxup ceo . n n xxmaj leave a comment or xxup rt . n n http : / / t.co / xxunk | negative | . 4 xxbos @southwestair xxmaj thx for the xxup grand view today ! xxup flt xxunk xxup seatac to xxup phx . # xxunk # xxmaj arizona # xxmaj wow # xxmaj love # xxmaj photography http : / / t.co / xxunk | positive | . learn_clas = text_classifier_learner(tweet_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() . learn_clas = learn_clas.load_encoder(&quot;finetuned&quot;) . . learn_clas.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.799796 | 0.627124 | 0.739082 | 00:20 | . learn_clas.freeze_to(-2) learn_clas.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.725965 | 0.618923 | 0.734220 | 00:21 | . learn_clas.freeze_to(-3) learn_clas.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.664376 | 0.550634 | 0.781218 | 00:20 | . learn_clas.unfreeze() learn_clas.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.527189 | 0.525152 | 0.792136 | 00:21 | . 1 | 0.472593 | 0.530672 | 0.791027 | 00:21 | . # As mentioned I did try this first # learn_clas.fit_one_cycle(10, 2e-2) . . learn_clas.show_results() . text category category_ . 0 xxbos @united xxmaj hi have a question re future xxmaj flight xxmaj booking xxmaj problems . xxup dub - jac 29 / 9 xxup jac - lax 8 / 10 xxup lax - dub 13 / 10 . xxmaj i &#39;m * xxunk xxmaj what is checked bag allowance for xxup jac - lax ? | neutral | negative | . 1 xxbos @usairways 4 segments , 4 / 4 delayed . xxmaj gnv &gt; xxup ctl . xxup ctl &gt; xxup jan . xxup jan &gt; xxup ctl . xxup ctl &gt; xxup gnv . xxmaj my year off from flying with you guys was the way to go . | negative | negative | . 2 xxbos @united xxup xxunk from xxup ric , xxup xxunk from xxup ord , &amp; &amp; xxup xxunk from xxup den xxunk xxup delayed for non - weather issues . xxmaj way to go , you re batting 1 . xxrep 3 0 ! xxmaj but no hotels | negative | negative | . 3 xxbos xxmaj can i put sun in my carry on ? xxup rt ‚Äú @united : xxunk xxrep 3 e xxmaj right now 0 would be a heat xxunk , so enjoy the warmth ! xxmaj can you bring some home ? xxunk ‚Äù | neutral | neutral | . 4 xxbos @united ( 2 / 2 ) xxmaj it xxunk that if ca n&#39;t confirm cert at time of xxmaj flight xxmaj booking xxmaj problems , i should assume that it may never clear . xxmaj did n&#39;t used to be that way | negative | negative | . 5 xxbos @americanair xxmaj yes i am . xxunk / xxunk . xxup rno departure at xxunk on 2 / 25 w / connection at xxup dfw to xxup lga . i can do the xxunk to xxup lax and then to xxup jfk | negative | neutral | . 6 xxbos @southwestair - xxmaj hi . xxmaj my flight confirmation # is xxunk . xxmaj we are currently stuck in xxmaj norfolk , xxmaj va . xxmaj trying to get to xxup lga in xxup nyc . xxmaj xxunk updates ? xxmaj thx | negative | neutral | . 7 xxbos @united booked award tix on xxunk but xxmaj i &#39;m not seeing a conf # for them , just the xxmaj united xxmaj flight xxmaj booking xxmaj problems # . xxmaj can you get get the xxmaj xxunk # for me ? | neutral | neutral | . 8 xxbos @usairways if you get my bag from e xxmaj gate to b xxmaj gate at the xxmaj charlotte airport before my 6 pm flight ( late xxmaj flight cause of y all ) xxmaj i ‚Äôll ride xxup us for life | negative | negative | . interp = ClassificationInterpretation.from_learner(learn_clas) interp.plot_confusion_matrix() . learn_clas.predict(&quot;I really loved that flight, it was awesome!&quot;) . . (&#39;positive&#39;, TensorText(2), TensorText([0.0047, 0.0024, 0.9929])) . #learn_clas.save(&#39;10epoch_clas&#39;) . .",
            "url": "https://thestiana.github.io/vigilant-telegram/2022/01/01/FastAi.html",
            "relUrl": "/2022/01/01/FastAi.html",
            "date": " ‚Ä¢ Jan 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://thestiana.github.io/vigilant-telegram/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://thestiana.github.io/vigilant-telegram/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://thestiana.github.io/vigilant-telegram/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}